# Loan Default Prediction App ğŸš€

This project predicts whether a loan applicant is likely to default or not, using machine learning models.  
Multiple algorithms were tested (Logistic Regression, Random Forest, XGBoost), and *XGBoost* was chosen as the final model for deployment due to its superior performance.  

## ğŸ”— Demo
- Streamlit App - https://huggingface.co/spaces/chandra1024/Loan-Default-Prediction

---

## ğŸ“Œ Features
- Exploratory Data Analysis (EDA) on loan dataset  
- Model training using Logistic Regression, Random Forest, and XGBoost  
- Feature importance visualization  
- Interactive *Streamlit web app* for real-time loan default predictions  

---

## ğŸ› ï¸ Tech Stack
- *Programming Language*: Python  
- *Libraries*: Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn  
- *Deployment*: Streamlit 
- *Model Persistence*: Pickle  

---

## ğŸ“‚ Project Workflow
1. Data preprocessing (missing values, encoding categorical features, scaling numeric values)  
2. Model training & evaluation on multiple algorithms  
3. Model selection â†’ XGBoost chosen as final model  
4. Saving trained model & encoders using Pickle  
5. Deployment via Streamlit (UI) 

---

## âš™ï¸ Installation & Usage
Clone this repository and install dependencies:

---

## **Run Streamlit App**
streamlit run app.py

---

## ğŸ“Š**Result & Insights**

ğŸ”¹ Model Performance (Default Threshold = 0.5)
	â€¢	Accuracy: ~72%
	â€¢	Precision (Defaulters = 1): 0.23
	â€¢	Recall (Defaulters = 1): 0.62
	â€¢	F1-Score (Defaulters = 1): 0.34

â¡ï¸ The model captures many defaulters (high recall) but also produces a high number of false positives (low precision).

â¸»

ğŸ”¹ After Threshold Optimization (Best = 0.65)
	â€¢	Accuracy: ~83%
	â€¢	Precision (Defaulters = 1): 0.31
	â€¢	Recall (Defaulters = 1): 0.41
	â€¢	F1-Score (Defaulters = 1): 0.36

â¡ï¸ Increasing the threshold improves accuracy and balances precision/recall better.

â¡ï¸ False positives reduce, while still catching a reasonable portion of defaulters.

---

**ğŸ”¹ Key Insights**

	â€¢	XGBoost outperformed Logistic Regression & Random Forest.
	â€¢	Threshold tuning significantly improved results for the minority class (defaulters).
	â€¢	Limitations:
	â€¢	Precision and recall for defaulters remain relatively low due to dataset imbalance.
	â€¢	Model may miss some defaulters, which can be costly in real banking scenarios.
	â€¢	Future focus: handling imbalance (SMOTE, class weights), cost-sensitive learning, and advanced models.

**âœ¨ Verdict:**
This is a strong baseline model (83% accuracy with threshold tuning).
Further improvements are needed for production-level deployment in financial institutions.

â¸»

**ğŸš€ Future Improvements**
	â€¢	Apply SMOTE / class weighting for better handling of imbalance
	â€¢	Perform advanced hyperparameter tuning (GridSearchCV / RandomizedSearchCV)
	â€¢	Deploy with Docker / Cloud (AWS, GCP, Azure)
	â€¢	Add monitoring for model drift in production
	â€¢	Explore ensemble stacking or deep learning methods

â¸»

**ğŸ“¸ Screenshots**

<img width="1896" height="861" alt="image" src="https://github.com/user-attachments/assets/6f1e3f10-f7c9-426d-b598-00ec5280b6f7" />

â¸»

**ğŸ¤ Contributing**

Contributions, issues, and feature requests are welcome!
Feel free to fork this repo and submit a pull request.

â¸»

**ğŸ“œ License**

This project is licensed under the MIT License â€“ see the LICENSE file for details.

â¸»

**ğŸ™‹â€â™€ï¸ About Me**

Hi, Iâ€™m Chandrakala Somanath Chippa ğŸ‘‹

	â€¢	ğŸ“ PGP in AIML from Austin University, Texas
 
	â€¢	ğŸ’¼ 7+ years experience in Finance (TCS, Gallagher, Propark Mobility)
 
	â€¢	ğŸ¤– Passionate about AI/ML, Data Science & Model Deployment
 
	â€¢	ğŸŒ [(https://www.linkedin.com/in/chandrakala-chippa-402529280/?trk=PROFILE_DROP_DOWN] |
 			[https://github.com/chandra1024448/loan-default-prediction/edit/main]

```bash
# Clone repo
git clone - https://github.com/chandra1024448/loan-default-prediction/new/main
cd loan-default-prediction

# Install dependencies
pip install -r requirements.txt

---



